### 2019.05.28.(화)_AI_Basic Machine Learning(1) 

------

#### Lec 00: Docker

- Docker란 : Container-based Virtualization System으로, 한 컴퓨터에서 여러 개의 운영체제를 구동하면 성능이 저하되는 것을 막기 위해, 알맹이(Linux Cernel)를 통일하고 어플리케이션 단(Containerized Application)을 가볍게 가상화한 것이다.

- 안내서 : <https://github.com/deeplearningzerotoall/TensorFlow/blob/master/docker_user_guide.md>

- IP : 192.168.99.100 (LGM)

- Docker Command 

  - Container 생성

    ```
    docker run -i -t --name tf -p 8888:8888 -p 6006:6006 deeplearningzerotoall/tensorflow
    ```

  - Container 상태 확인

    ```
    docker ps -a
    ```
  
  - Container 실행

    ```
    docker start [name]
    ```
    
  - Container 접속

    ```
    docker attach [name]
    ```
------

#### Lec 01: 기본적인 Machine Learning 의 용어와 개념 설명

- 머신러닝 : Arthur Samuel(1959) / 컴퓨터가 배우고 학습하는 능력을 갖는것

- Supervised Learning : 정해져 있는 데이터(training set)을 기반으로 학습하는 방법

  - Training data set : 머신러닝 모델이 학습하는 데이터

    ![img](../resources/img/2019.05.28.(화)/img-1.png)

  - Regression(회귀) : 예측 값이 float 형태일 때 예측하는 것 (시험점수 예측)

    ![img](../resources/img/2019.05.28.(화)/img-2.png)

  - Binary classification : pass/non-pass를 예측하는 것

    ![img](../resources/img/2019.05.28.(화)/img-3.png)

  - Multi-label classification : Binary classification과 하나만 선택하는 것은 동일하지만, label이 여러개 있음 (학점 예측)

    ![img](../resources/img/2019.05.28.(화)/img-4.png)

    

- Unsupervised Learning : un-labeled data로 학습하는 방법 (Google news grouping, Word clustering)

------

#### Lec 02: Simple Linear Regression

- Regression 

  - "Regression toward the mean" (Sir Francis Galton)
  - 굉장히 크거나 작은 어떤 데이터는 전체의 평균으로 회귀하려는 특징이 있다.

- Linear Regression : 데이터를 잘 대변하는 직선 방정식을 찾는 것

  ![img](../resources/img/2019.05.28.(화)/img-5.png)

  - 파란 점선이 3점을 잘 대변하고 있는가?

    ![img](../resources/img/2019.05.28.(화)/img-6.png)

  - 이때 직선 방정식과 점들과의 차이를 Cost라고 부르는데, 이 Cost가 작을 수록 잘 대변하고 있는 것이다.

    ![img](../resources/img/2019.05.28.(화)/img-7.png)

  - 식이 어떻게 유도 되는가 생각해보기

  - 우리의 목표 :  Minimize cost

    ![img](../resources/img/2019.05.28.(화)/img-8.png)

------

#### Lab 02: Simple Linear Regression 를 TensorFlow 로 구현하기

- Build hypothesis and cost

  ![img](../resources/img/2019.05.28.(화)/img-9.png)

  - reduce : 차원(rank)를 줄인다.

    ![img](../resources/img/2019.05.28.(화)/img-10.png)

  - 경사 하강법/경사 알고리즘

    ![img](../resources/img/2019.05.28.(화)/img-11.png)

    - cost가 최소가 되는 W와 b를 찾는 알고리즘
    - learning_rate : 굉장히 작은 값을 사용함 / 구한 기울기를 얼만큼 반영할 것인가를 결정하는 변수

  - 전체 코드

    ```
    import tensorflow as tf
    
    # tensorflow 즉시 실행 활성화
    tf.enable_eager_execution() 
    
    # Data 
    x_data = [1, 2, 3, 4, 5]
    y_data = [1, 2, 3, 4, 5]
    
    # W, b initialize 
    W = tf.Variable(2.9)
    b = tf.Variable(0.5)
    
    learning_rate = 0.01
    
    for i in range(100+1): # W, b update    
        # Gradient descent   
        with tf.GradientTape() as tape: 
            hypothesis = W * x_data + b   
            cost = tf.reduce_mean(tf.square(hypothesis - y_data))
            W_grad, b_grad = tape.gradient(cost, [W, b]) 
            W.assign_sub(learning_rate * W_grad)  
            b.assign_sub(learning_rate * b_grad)  
            if i % 10 == 0:    
                print("{:5}|{:10.4f}|{:10.4}|{:10.6f}".format(i, W.numpy(), b.numpy(), cost))
    ```
    - Output

      ![img](../resources/img/2019.05.28.(화)/img-13.png)

  - Training

    ![img](../resources/img/2019.05.28.(화)/img-14.png)

  - 훈련된 Hypothesis로 값 예측하기 / 거의 유사하게 나오는 것을 확인할 수 있음

    ![img](../resources/img/2019.05.28.(화)/img-15.png)

---

#### Lec 03: Linear Regression and How to minimize cost

- 기존의 Hypothesis와 Cost를 간략화

  ![img](../resources/img/2019.05.28.(화)/img-16.png)

- Hypothesis에 따른 cost 변화

  ![img](../resources/img/2019.05.28.(화)/img-17.png)

- Gradient descent algorithm 동작 과정

  ![img](../resources/img/2019.05.28.(화)/img-18.png)

  ![img](../resources/img/2019.05.28.(화)/img-19.png)
  - 한 step을 움직일 때, 기울기 비례로 이동한다. 즉 기울기가 가파를 수록 더욱 많이 이동한다.

  - Gradient는 미분으로 구할 때, 간편한 계산을 위한 식 변화 (평균을 구할때 분모의 계수는 몇이여도 상관없는데, 미분했을때 약분되게 하기 위해 2로 설정)

    ![img](../resources/img/2019.05.28.(화)/img-20.png)

  - 미분 과정 (알파 = learning rate)

    ![img](../resources/img/2019.05.28.(화)/img-21.png)

  - Gradient descent 한계점 (다음과 같은 그림에서는 시작점에 따라 최적의 cost를 찾을 수 없다.)

    ![img](../resources/img/2019.05.28.(화)/img-22.png)

  - 다음 그림에서만 가능함

    ![img](../resources/img/2019.05.28.(화)/img-23.png)

---

#### Lab 03: Linear Regression and How to minimize cost 를 TensorFlow 로 구현하기

- cost는 0으로 수렴하고, W는 특정 값으로 수렴하는 것을 알 수 있다.

  ![img](../resources/img/2019.05.28.(화)/img-24.png)

---

